# AI Agents â€“ Notion-Integrated Automation Toolkit

A growing collection of command-line "agents" that plug directly into your Notion workspace.  Each agent automates a specific workflow â€“ starting today with a **Resume Tailoring** agent and with many more to come.

---

## ğŸš€ Quick Start

```bash
# 1â€€Create env (Python 3.12) and activate
conda create -n ai-agents python=3.12 -y
conda activate ai-agents

# 2â€€Install project
pip install -e .

# 3â€€Configure secrets (OpenAI & Notion)
cp .env.example .env && $EDITOR .env

# 4â€€Initialise / repair the Notion DB
python src/main.py resume init

# 5â€€Extract metadata from a job ad
python src/main.py resume extract "https://careers.example.com/jobs/1234"

# 6â€€Generate a laser-focused PDF resume (uses the metadata now stored in Notion, *not* the raw job ad)
python src/main.py resume tailor "https://careers.example.com/jobs/1234"
```

> **Customize** â€“ tweak `data/template.tex` and/or add new properties to the Notion DB. The agent will pick them up automatically.

---

## ğŸ¤– Available Agents

<details open>
<summary><strong>Resume Tailoring Agent</strong> (currently the only agent)</summary>

| Command | Description |
|---------|-------------|
| `resume init` | Verifies **and fixes** the configured Notion database schema. Run this once or after you modify the DB. |
| `resume extract <job_url>` | Scrapes & analyses the job posting, producing structured metadata and saving it to Notion. |
| `resume tailor <job_url>` | Creates a PDF resume tailored to the job (based on `template.tex`) and uploads it to Notion. |

The Resume Tailoring agent exposes three sub-commands:

* **init** â€“ verifies (and automatically repairs) your Notion database schema. Run this once (or any time you change the DB).
* **extract &lt;job_url&gt;** â€“ scrapes & analyses a job posting, then saves rich, structured metadata back to Notion.
* **tailor &lt;job_url&gt;** â€“ renders `data/template.tex` into a PDF resume **solely using the metadata stored in Notion**. (The richer the metadata â€“ e.g. "Key Achievements", "Core Competencies", "Tech Stack" â€“ the better the tailoring quality.)

Run any of the above with:

```bash
python src/main.py resume <command> [...]
```

</details>

Future agents (e.g. *Job-Application Tracker*, *Content Planner*, â€¦) will be added under their own top-level command (`python src/main.py <agent> <command>`).

---

## ğŸ“ Property Description Directives

When you create properties in your Notion database you can add special tags in the *description* field to control how they're treated by the agents:

* `#exclude` â€“ exclude the property from the AI JSON schema (useful for internal fields like status flags, URLs, etc.).
* `#keep-options` â€“ for `select` / `multi_select` / `status` types, always include the option list as an enum even when `--add-properties-options` is false.

> **Tip:** Add additional context-rich properties (e.g. *Key Achievements*, *Core Competencies*, *Mission Statement*) to your Notion page. The more context the LLM has, the better it can tailor your resume.

---

## âœ¨ Features

<details>
<summary>Click to view full list</summary>

* AI-powered metadata extraction (GPT-4o-mini by default)
* Seamless Notion API integration (schema validation, file uploads, property mapping)
* LaTeX âœ PDF compilation pipeline with automatic Notion upload
* Rich-style logging & Pydantic configuration
* 100 % typed codebase with Ruff, Black and MyPy pre-commit hooks
* Extensive unit-test suite & fixtures

</details>

---

## ğŸ—‚ï¸ Key Project Structure

```
ai_agents/
â”œâ”€ data/                 # LaTeX template & prompt files
â”œâ”€ src/
â”‚  â”œâ”€ common/            # Shared services (Notion, OpenAI, â€¦)
â”‚  â”œâ”€ metadata_extraction/  # Job-posting analysis
â”‚  â”œâ”€ resume_tailoring/     # LaTeX rendering & PDF compiler
â”‚  â””â”€ main.py            # CLI entry-point (python src/main.py ...)
â””â”€ tests/                # Pytest suite
```

See `design_docs/` for deep-dive architecture notes.

---

## ğŸ› ï¸ Development

```bash
# 0â€€(one-time) install *dev* extras and pre-commit hooks
pip install -e .[dev]
pre-commit install

# 1â€€Run the full test + lint stack
pytest -v
ruff format .    # auto-format all files
ruff check .     # lint
mypy src tests   # static type checks

# 2â€€Run pre-commit on all files (optional but handy)
pre-commit run --all-files
```

---

## ğŸ“„ License

Apache 2.0 â€“ see `LICENSE` for details.
